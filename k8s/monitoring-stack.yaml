# PersonaPass Monitoring Stack - ELK, Prometheus, Grafana
# Comprehensive observability platform for production environments

apiVersion: v1
kind: Namespace
metadata:
  name: personapass-monitoring
  labels:
    name: personapass-monitoring
    purpose: observability
---
# Elasticsearch for log aggregation
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: elasticsearch
  namespace: personapass-monitoring
  labels:
    app: elasticsearch
spec:
  serviceName: elasticsearch-headless
  replicas: 3
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 1000
      initContainers:
      - name: increase-vm-max-map
        image: busybox:1.35
        command: ['sh', '-c', 'sysctl -w vm.max_map_count=262144']
        securityContext:
          privileged: true
      containers:
      - name: elasticsearch
        image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
        ports:
        - containerPort: 9200
          name: http
        - containerPort: 9300
          name: transport
        env:
        - name: cluster.name
          value: "personapass-logs"
        - name: node.name
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: discovery.seed_hosts
          value: "elasticsearch-headless"
        - name: cluster.initial_master_nodes
          value: "elasticsearch-0,elasticsearch-1,elasticsearch-2"
        - name: ES_JAVA_OPTS
          value: "-Xms2g -Xmx2g"
        - name: xpack.security.enabled
          value: "true"
        - name: xpack.security.transport.ssl.enabled
          value: "true"
        - name: xpack.security.transport.ssl.verification_mode
          value: "certificate"
        - name: xpack.security.transport.ssl.keystore.path
          value: "/usr/share/elasticsearch/config/certs/elastic-stack-ca.p12"
        - name: xpack.security.transport.ssl.truststore.path
          value: "/usr/share/elasticsearch/config/certs/elastic-stack-ca.p12"
        resources:
          requests:
            memory: "3Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
        - name: elasticsearch-config
          mountPath: /usr/share/elasticsearch/config/elasticsearch.yml
          subPath: elasticsearch.yml
      volumes:
      - name: elasticsearch-config
        configMap:
          name: elasticsearch-config
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: "fast-ssd"
      resources:
        requests:
          storage: 50Gi

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: elasticsearch-config
  namespace: personapass-monitoring
data:
  elasticsearch.yml: |
    cluster.name: "personapass-logs"
    network.host: 0.0.0.0
    path.data: /usr/share/elasticsearch/data
    path.logs: /usr/share/elasticsearch/logs
    xpack.security.enabled: true
    xpack.monitoring.collection.enabled: true
    indices.memory.index_buffer_size: 30%
    thread_pool.write.queue_size: 1000

---
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch-headless
  namespace: personapass-monitoring
spec:
  clusterIP: None
  ports:
  - port: 9200
    name: http
  - port: 9300
    name: transport
  selector:
    app: elasticsearch

---
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch
  namespace: personapass-monitoring
spec:
  ports:
  - port: 9200
    name: http
  selector:
    app: elasticsearch

---
# Kibana for log visualization
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kibana
  namespace: personapass-monitoring
  labels:
    app: kibana
spec:
  replicas: 2
  selector:
    matchLabels:
      app: kibana
  template:
    metadata:
      labels:
        app: kibana
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
      containers:
      - name: kibana
        image: docker.elastic.co/kibana/kibana:8.11.0
        ports:
        - containerPort: 5601
        env:
        - name: ELASTICSEARCH_HOSTS
          value: "http://elasticsearch:9200"
        - name: SERVER_NAME
          value: "kibana.personapass.io"
        - name: SERVER_BASEPATH
          value: "/kibana"
        - name: XPACK_SECURITY_ENABLED
          value: "true"
        - name: XPACK_MONITORING_ENABLED
          value: "true"
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        readinessProbe:
          httpGet:
            path: /api/status
            port: 5601
          initialDelaySeconds: 30
          periodSeconds: 10

---
apiVersion: v1
kind: Service
metadata:
  name: kibana
  namespace: personapass-monitoring
spec:
  ports:
  - port: 5601
    targetPort: 5601
  selector:
    app: kibana

---
# Logstash for log processing
apiVersion: apps/v1
kind: Deployment
metadata:
  name: logstash
  namespace: personapass-monitoring
  labels:
    app: logstash
spec:
  replicas: 2
  selector:
    matchLabels:
      app: logstash
  template:
    metadata:
      labels:
        app: logstash
    spec:
      containers:
      - name: logstash
        image: docker.elastic.co/logstash/logstash:8.11.0
        ports:
        - containerPort: 5044
        - containerPort: 9600
        env:
        - name: LS_JAVA_OPTS
          value: "-Xmx2g -Xms2g"
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "3Gi"
            cpu: "2000m"
        volumeMounts:
        - name: logstash-config
          mountPath: /usr/share/logstash/pipeline
      volumes:
      - name: logstash-config
        configMap:
          name: logstash-config

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: logstash-config
  namespace: personapass-monitoring
data:
  logstash.conf: |
    input {
      beats {
        port => 5044
      }
      http {
        port => 8080
        codec => json
      }
    }
    
    filter {
      if [kubernetes] {
        mutate {
          add_field => { "service" => "%{[kubernetes][labels][app]}" }
          add_field => { "namespace" => "%{[kubernetes][namespace]}" }
          add_field => { "pod" => "%{[kubernetes][pod][name]}" }
        }
      }
      
      # Parse PersonaPass specific logs
      if [service] == "personapass-wallet" {
        grok {
          match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} %{GREEDYDATA:log_message}" }
        }
        
        # Parse performance metrics
        if [log_message] =~ /performance/ {
          grok {
            match => { "log_message" => "performance: %{DATA:metric_name}=%{NUMBER:metric_value}" }
          }
        }
        
        # Parse security events
        if [log_message] =~ /security/ {
          mutate { add_tag => ["security"] }
          grok {
            match => { "log_message" => "security: %{DATA:event_type} user=%{DATA:user_id} ip=%{IP:client_ip}" }
          }
        }
      }
      
      if [service] == "personapass-blockchain" {
        # Parse Tendermint logs
        grok {
          match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} %{WORD:module} %{LOGLEVEL:level} %{GREEDYDATA:log_message}" }
        }
        
        # Parse consensus events
        if [module] == "consensus" {
          mutate { add_tag => ["consensus"] }
        }
        
        # Parse P2P events
        if [module] == "p2p" {
          mutate { add_tag => ["p2p"] }
        }
      }
      
      # Parse ZK proof operations
      if [log_message] =~ /zk_proof/ {
        mutate { add_tag => ["zk_proof"] }
        grok {
          match => { "log_message" => "zk_proof: %{DATA:operation} type=%{DATA:proof_type} duration=%{NUMBER:duration_ms}ms" }
        }
      }
      
      # Geolocate IP addresses
      if [client_ip] {
        geoip {
          source => "client_ip"
          target => "geoip"
        }
      }
      
      # Add custom fields
      mutate {
        add_field => { "environment" => "production" }
        add_field => { "platform" => "personapass" }
      }
      
      date {
        match => [ "timestamp", "ISO8601" ]
      }
    }
    
    output {
      elasticsearch {
        hosts => ["elasticsearch:9200"]
        index => "personapass-logs-%{+YYYY.MM.dd}"
        template_name => "personapass"
        template_pattern => "personapass-*"
        template => {
          "index_patterns" => ["personapass-*"]
          "mappings" => {
            "properties" => {
              "@timestamp" => { "type" => "date" }
              "service" => { "type" => "keyword" }
              "namespace" => { "type" => "keyword" }
              "level" => { "type" => "keyword" }
              "metric_name" => { "type" => "keyword" }
              "metric_value" => { "type" => "float" }
              "event_type" => { "type" => "keyword" }
              "user_id" => { "type" => "keyword" }
              "client_ip" => { "type" => "ip" }
              "proof_type" => { "type" => "keyword" }
              "duration_ms" => { "type" => "float" }
              "geoip" => {
                "properties" => {
                  "location" => { "type" => "geo_point" }
                  "country_name" => { "type" => "keyword" }
                  "city_name" => { "type" => "keyword" }
                }
              }
            }
          }
        }
      }
      
      # Send critical errors to alerting
      if [level] == "ERROR" or "security" in [tags] {
        http {
          url => "http://alertmanager:9093/api/v1/alerts"
          http_method => "post"
          format => "json"
          mapping => {
            "alerts" => [
              {
                "labels" => {
                  "alertname" => "LogAlert"
                  "service" => "%{service}"
                  "severity" => "warning"
                  "instance" => "%{pod}"
                }
                "annotations" => {
                  "summary" => "Error detected in %{service}"
                  "description" => "%{log_message}"
                }
              }
            ]
          }
        }
      }
    }

---
apiVersion: v1
kind: Service
metadata:
  name: logstash
  namespace: personapass-monitoring
spec:
  ports:
  - port: 5044
    name: beats
  - port: 8080
    name: http
  selector:
    app: logstash

---
# Filebeat DaemonSet for log collection
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: filebeat
  namespace: personapass-monitoring
  labels:
    app: filebeat
spec:
  selector:
    matchLabels:
      app: filebeat
  template:
    metadata:
      labels:
        app: filebeat
    spec:
      serviceAccountName: filebeat
      terminationGracePeriodSeconds: 30
      hostNetwork: true
      dnsPolicy: ClusterFirstWithHostNet
      containers:
      - name: filebeat
        image: docker.elastic.co/beats/filebeat:8.11.0
        args: [
          "-c", "/etc/filebeat.yml",
          "-e",
        ]
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        securityContext:
          runAsUser: 0
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 100Mi
        volumeMounts:
        - name: config
          mountPath: /etc/filebeat.yml
          readOnly: true
          subPath: filebeat.yml
        - name: data
          mountPath: /usr/share/filebeat/data
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: varlog
          mountPath: /var/log
          readOnly: true
      volumes:
      - name: config
        configMap:
          defaultMode: 0640
          name: filebeat-config
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: varlog
        hostPath:
          path: /var/log
      - name: data
        hostPath:
          path: /var/lib/filebeat-data
          type: DirectoryOrCreate

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: filebeat-config
  namespace: personapass-monitoring
data:
  filebeat.yml: |-
    filebeat.inputs:
    - type: container
      paths:
        - /var/log/containers/*.log
      processors:
        - add_kubernetes_metadata:
            host: ${NODE_NAME}
            matchers:
            - logs_path:
                logs_path: "/var/log/containers/"

    # Additional file inputs for system logs
    - type: log
      paths:
        - /var/log/auth.log
        - /var/log/secure
      fields:
        logtype: system
        
    processors:
    - add_host_metadata:
        when.not.contains.tags: forwarded
    - add_docker_metadata: ~
    - add_kubernetes_metadata:
        host: ${NODE_NAME}
        matchers:
        - logs_path:
            logs_path: "/var/log/containers/"

    # Enrich with labels
    - add_labels:
        labels:
          environment: production
          datacenter: us-east-1

    # Drop certain logs to reduce noise
    - drop_event:
        when:
          regexp:
            message: '(health|ping|GET /health)'

    output.logstash:
      hosts: ["logstash:5044"]
      
    setup.template.settings:
      index.number_of_shards: 3
      index.number_of_replicas: 1
      
    logging.level: info

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: filebeat
  namespace: personapass-monitoring

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: filebeat
rules:
- apiGroups: [""]
  resources:
  - nodes
  - namespaces
  - events
  - pods
  verbs: ["get", "list", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: filebeat
subjects:
- kind: ServiceAccount
  name: filebeat
  namespace: personapass-monitoring
roleRef:
  kind: ClusterRole
  name: filebeat
  apiGroup: rbac.authorization.k8s.io

---
# Enhanced Prometheus with PersonaPass-specific configuration
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: prometheus
  namespace: personapass-monitoring
  labels:
    app: prometheus
spec:
  serviceName: prometheus-headless
  replicas: 2
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      serviceAccountName: prometheus
      securityContext:
        runAsUser: 65534
        fsGroup: 65534
      containers:
      - name: prometheus
        image: prom/prometheus:v2.48.0
        args:
        - '--config.file=/etc/prometheus/prometheus.yml'
        - '--storage.tsdb.path=/prometheus'
        - '--storage.tsdb.retention.time=30d'
        - '--storage.tsdb.retention.size=50GB'
        - '--web.console.libraries=/etc/prometheus/console_libraries'
        - '--web.console.templates=/etc/prometheus/consoles'
        - '--web.enable-lifecycle'
        - '--web.enable-admin-api'
        - '--web.external-url=https://monitoring.personapass.io/prometheus'
        - '--web.route-prefix=/'
        ports:
        - containerPort: 9090
          name: web
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
          limits:
            memory: "8Gi"
            cpu: "4000m"
        volumeMounts:
        - name: prometheus-config
          mountPath: /etc/prometheus
        - name: prometheus-storage
          mountPath: /prometheus
        livenessProbe:
          httpGet:
            path: /-/healthy
            port: 9090
          initialDelaySeconds: 30
          timeoutSeconds: 30
        readinessProbe:
          httpGet:
            path: /-/ready
            port: 9090
          initialDelaySeconds: 30
          timeoutSeconds: 30
      volumes:
      - name: prometheus-config
        configMap:
          name: prometheus-config
  volumeClaimTemplates:
  - metadata:
      name: prometheus-storage
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: "fast-ssd"
      resources:
        requests:
          storage: 100Gi

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
  namespace: personapass-monitoring

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus
rules:
- apiGroups: [""]
  resources: ["nodes", "nodes/proxy", "services", "endpoints", "pods"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["extensions"]
  resources: ["ingresses"]
  verbs: ["get", "list", "watch"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
- kind: ServiceAccount
  name: prometheus
  namespace: personapass-monitoring

---
apiVersion: v1
kind: Service
metadata:
  name: prometheus
  namespace: personapass-monitoring
spec:
  ports:
  - port: 9090
    targetPort: 9090
  selector:
    app: prometheus

---
# Grafana with PersonaPass dashboards
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: personapass-monitoring
  labels:
    app: grafana
spec:
  replicas: 2
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      securityContext:
        runAsUser: 472
        fsGroup: 472
      containers:
      - name: grafana
        image: grafana/grafana:10.2.0
        ports:
        - containerPort: 3000
        env:
        - name: GF_SECURITY_ADMIN_PASSWORD
          valueFrom:
            secretRef:
              name: grafana-admin
              key: password
        - name: GF_USERS_ALLOW_SIGN_UP
          value: "false"
        - name: GF_SERVER_ROOT_URL
          value: "https://monitoring.personapass.io/grafana"
        - name: GF_SERVER_SERVE_FROM_SUB_PATH
          value: "true"
        - name: GF_AUTH_ANONYMOUS_ENABLED
          value: "false"
        - name: GF_INSTALL_PLUGINS
          value: "grafana-piechart-panel,grafana-worldmap-panel,grafana-clock-panel"
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        volumeMounts:
        - name: grafana-storage
          mountPath: /var/lib/grafana
        - name: grafana-provisioning
          mountPath: /etc/grafana/provisioning
        readinessProbe:
          httpGet:
            path: /api/health
            port: 3000
          initialDelaySeconds: 30
          periodSeconds: 10
      volumes:
      - name: grafana-storage
        persistentVolumeClaim:
          claimName: grafana-storage
      - name: grafana-provisioning
        configMap:
          name: grafana-provisioning

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: grafana-storage
  namespace: personapass-monitoring
spec:
  accessModes: ["ReadWriteOnce"]
  storageClassName: "standard-ssd"
  resources:
    requests:
      storage: 10Gi

---
apiVersion: v1
kind: Service
metadata:
  name: grafana
  namespace: personapass-monitoring
spec:
  ports:
  - port: 3000
    targetPort: 3000
  selector:
    app: grafana

---
# Ingress for monitoring services
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: monitoring-ingress
  namespace: personapass-monitoring
  annotations:
    kubernetes.io/ingress.class: "nginx"
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/auth-type: basic
    nginx.ingress.kubernetes.io/auth-secret: monitoring-auth
spec:
  tls:
  - hosts:
    - monitoring.personapass.io
    secretName: monitoring-tls
  rules:
  - host: monitoring.personapass.io
    http:
      paths:
      - path: /grafana
        pathType: Prefix
        backend:
          service:
            name: grafana
            port:
              number: 3000
      - path: /prometheus
        pathType: Prefix
        backend:
          service:
            name: prometheus
            port:
              number: 9090
      - path: /kibana
        pathType: Prefix
        backend:
          service:
            name: kibana
            port:
              number: 5601